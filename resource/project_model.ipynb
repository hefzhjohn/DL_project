{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nutritional-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "df = pd.read_csv(r'processed_data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beautiful-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['optionid','date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "comparative-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['optionid_chg'] = df['optionid'].diff(periods=-1) \n",
    "df['impvol_chg'] = df['impl_volatility'].diff(periods=-1) * 100\n",
    "df['ret_chg'] = df['Index_Spot'].pct_change(periods=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "animal-sarah",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31894768"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "delayed-toddler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>optionid</th>\n",
       "      <th>date</th>\n",
       "      <th>exdate</th>\n",
       "      <th>strike_price</th>\n",
       "      <th>best_bid</th>\n",
       "      <th>best_offer</th>\n",
       "      <th>impl_volatility</th>\n",
       "      <th>delta</th>\n",
       "      <th>days_to_expiry</th>\n",
       "      <th>time_to_maturity</th>\n",
       "      <th>Index_Spot</th>\n",
       "      <th>optionid_chg</th>\n",
       "      <th>impvol_chg</th>\n",
       "      <th>ret_chg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>2386060</td>\n",
       "      <td>33033967.0</td>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>2010-12-18</td>\n",
       "      <td>700000</td>\n",
       "      <td>422.3</td>\n",
       "      <td>426.3</td>\n",
       "      <td>0.331839</td>\n",
       "      <td>0.927833</td>\n",
       "      <td>348</td>\n",
       "      <td>0.953425</td>\n",
       "      <td>1132.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3542</td>\n",
       "      <td>-0.003106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>2387944</td>\n",
       "      <td>33033967.0</td>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>2010-12-18</td>\n",
       "      <td>700000</td>\n",
       "      <td>424.9</td>\n",
       "      <td>428.9</td>\n",
       "      <td>0.328297</td>\n",
       "      <td>0.930406</td>\n",
       "      <td>347</td>\n",
       "      <td>0.950685</td>\n",
       "      <td>1136.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1979</td>\n",
       "      <td>-0.000545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>2389832</td>\n",
       "      <td>33033967.0</td>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>2010-12-18</td>\n",
       "      <td>700000</td>\n",
       "      <td>425.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>0.326318</td>\n",
       "      <td>0.931496</td>\n",
       "      <td>346</td>\n",
       "      <td>0.947945</td>\n",
       "      <td>1137.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.3351</td>\n",
       "      <td>-0.003985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>2391720</td>\n",
       "      <td>33033967.0</td>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>2010-12-18</td>\n",
       "      <td>700000</td>\n",
       "      <td>429.6</td>\n",
       "      <td>433.6</td>\n",
       "      <td>0.329669</td>\n",
       "      <td>0.931657</td>\n",
       "      <td>345</td>\n",
       "      <td>0.945205</td>\n",
       "      <td>1141.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8194</td>\n",
       "      <td>-0.002873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>2393585</td>\n",
       "      <td>33033967.0</td>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>2010-12-18</td>\n",
       "      <td>700000</td>\n",
       "      <td>431.6</td>\n",
       "      <td>435.5</td>\n",
       "      <td>0.321475</td>\n",
       "      <td>0.935998</td>\n",
       "      <td>344</td>\n",
       "      <td>0.942466</td>\n",
       "      <td>1144.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>-0.001744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Unnamed: 0.1    optionid        date      exdate  \\\n",
       "30          30       2386060  33033967.0  2010-01-04  2010-12-18   \n",
       "21          21       2387944  33033967.0  2010-01-05  2010-12-18   \n",
       "15          15       2389832  33033967.0  2010-01-06  2010-12-18   \n",
       "76          76       2391720  33033967.0  2010-01-07  2010-12-18   \n",
       "91          91       2393585  33033967.0  2010-01-08  2010-12-18   \n",
       "\n",
       "    strike_price  best_bid  best_offer  impl_volatility     delta  \\\n",
       "30        700000     422.3       426.3         0.331839  0.927833   \n",
       "21        700000     424.9       428.9         0.328297  0.930406   \n",
       "15        700000     425.0       429.0         0.326318  0.931496   \n",
       "76        700000     429.6       433.6         0.329669  0.931657   \n",
       "91        700000     431.6       435.5         0.321475  0.935998   \n",
       "\n",
       "    days_to_expiry  time_to_maturity  Index_Spot  optionid_chg  impvol_chg  \\\n",
       "30             348          0.953425     1132.99           0.0      0.3542   \n",
       "21             347          0.950685     1136.52           0.0      0.1979   \n",
       "15             346          0.947945     1137.14           0.0     -0.3351   \n",
       "76             345          0.945205     1141.69           0.0      0.8194   \n",
       "91             344          0.942466     1144.98           0.0     -0.3427   \n",
       "\n",
       "     ret_chg  \n",
       "30 -0.003106  \n",
       "21 -0.000545  \n",
       "15 -0.003985  \n",
       "76 -0.002873  \n",
       "91 -0.001744  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df[df[\"optionid_chg\"]==0]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "widespread-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, test_size=0.3)\n",
    "train = data[:int(len(data)*0.7)]\n",
    "test= data.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "located-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Xy(data, reshape):\n",
    "    X1 = data[\"ret_chg\"]/np.sqrt(data[\"time_to_maturity\"]);\n",
    "    X2 = X1 * data[\"delta\"]\n",
    "    X3 = X1 * data[\"delta\"] * data[\"delta\"]\n",
    "    X = np.array([X1,X2,X3])\n",
    "    if reshape:\n",
    "        X = X.reshape([len(X1),3])\n",
    "    \n",
    "    y = data[\"impvol_chg\"]\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "medical-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generate_Xy(train, True)\n",
    "X_test, y_test = generate_Xy(test, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "combined-bicycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef:  [ 0.07573041  0.02941734 -0.02501317]\n",
      "score:  -0.0024138730415819154\n"
     ]
    }
   ],
   "source": [
    "Reg = LinearRegression(fit_intercept=False)\n",
    "heston_model = Reg.fit(X_train, y_train)\n",
    "coef = heston_model.coef_\n",
    "score = heston_model.score(X_train,y_train)\n",
    "print( \"coef: \", coef )\n",
    "print( \"score: \", score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "powerful-sierra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812543069159875\n"
     ]
    }
   ],
   "source": [
    "y_pred = heston_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "further-trademark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:             impvol_chg   R-squared (uncentered):                   0.000\n",
      "Model:                            OLS   Adj. R-squared (uncentered):             -0.000\n",
      "Method:                 Least Squares   F-statistic:                             0.7037\n",
      "Date:                Sat, 27 Nov 2021   Prob (F-statistic):                       0.550\n",
      "Time:                        17:35:10   Log-Likelihood:                     -2.1191e+06\n",
      "No. Observations:             1359638   AIC:                                  4.238e+06\n",
      "Df Residuals:                 1359635   BIC:                                  4.238e+06\n",
      "Df Model:                           3                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.0757      0.059      1.294      0.196      -0.039       0.190\n",
      "x2             0.0294      0.059      0.502      0.616      -0.085       0.144\n",
      "x3            -0.0250      0.058     -0.428      0.669      -0.140       0.090\n",
      "==============================================================================\n",
      "Omnibus:                  2180995.957   Durbin-Watson:                   2.315\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):      12935998052.742\n",
      "Skew:                          -9.636   Prob(JB):                         0.00\n",
      "Kurtosis:                     480.464   Cond. No.                         1.01\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "result = sm.OLS(y_train, X_train).fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "atomic-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = np.array([data['ret_chg'],data[\"time_to_maturity\"],data[\"delta\"],data['impvol_chg']])\n",
    "dim_x, dim_y = data_set.shape\n",
    "data_set = data_set.reshape(dim_y, dim_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch mlp for binary classification\n",
    "\n",
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, df):\n",
    "        self.X = df[:,:-1]\n",
    "        self.y = df[:,-1]\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_inputs, 10)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(10, 8)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.hidden3 = Linear(8, 1)\n",
    "        self.act3 = Sigmoid()\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X\n",
    "\n",
    "# prepare the dataset\n",
    "def prepare_data(df):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(df)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=1024, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024*2, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    "\n",
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = BCELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(10):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "            if( i%50 == 0 ):\n",
    "                print(loss.data.item())\n",
    "\n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round to class values\n",
    "        yhat = yhat.round()\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc\n",
    "\n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat\n",
    "\n",
    "# prepare the data\n",
    "#path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
    "train_dl, test_dl = prepare_data(data_set)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "aging-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import vstack\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "attractive-profession",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1301368 640973\n"
     ]
    }
   ],
   "source": [
    "# pytorch mlp for binary classification\n",
    "\n",
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, df):\n",
    "        # load the csv file as a dataframe\n",
    "        #df = read_csv(path, header=None)\n",
    "        # store the inputs and outputs\n",
    "        self.X = df[:,:-1]\n",
    "        self.y = df[:,-1]\n",
    "        # ensure input data is floats\n",
    "        self.X = self.X.astype('float32')\n",
    "        # label encode target and ensure the values are floats\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    "        self.y = self.y.astype('float32')\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 10)\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = ReLU()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(10, 8)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(8, 1)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        self.act3 = Sigmoid()\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X\n",
    "\n",
    "# prepare the dataset\n",
    "def prepare_data(df):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(df)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=1024, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024*2, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    "\n",
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = BCELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(10):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "            if( i%50 == 0 ):\n",
    "                print(loss.data.item())\n",
    "\n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round to class values\n",
    "        yhat = yhat.round()\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc\n",
    "\n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat\n",
    "\n",
    "# prepare the data\n",
    "#path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
    "train_dl, test_dl = prepare_data(data_set)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "regular-davis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2047.8370607028753"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "640973/len(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "several-treat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11423.9189453125\n",
      "-16847214.0\n",
      "-16169500.0\n",
      "-17007368.0\n",
      "-16552789.0\n",
      "-15686357.0\n",
      "-17142342.0\n",
      "-17155090.0\n",
      "-16974560.0\n",
      "-16070769.0\n",
      "-16615280.0\n",
      "-17798986.0\n",
      "-16687104.0\n",
      "-17208956.0\n",
      "-16725684.0\n",
      "-16342250.0\n",
      "-16464888.0\n",
      "-16925782.0\n",
      "-16850008.0\n",
      "-15791834.0\n",
      "-17184020.0\n",
      "-15870552.0\n",
      "-16148444.0\n",
      "-16931908.0\n",
      "-17200798.0\n",
      "-16816460.0\n",
      "-16769114.0\n",
      "-16410886.0\n",
      "-16626166.0\n",
      "-17039988.0\n",
      "-16417505.0\n",
      "-17243642.0\n",
      "-16807790.0\n",
      "-16589477.0\n",
      "-17077648.0\n",
      "-16663139.0\n",
      "-16863522.0\n",
      "-16303840.0\n",
      "-17104368.0\n",
      "-16514328.0\n",
      "-17189004.0\n",
      "-16679528.0\n",
      "-16056410.0\n",
      "-16870116.0\n",
      "-16368366.0\n",
      "-16916710.0\n",
      "-16468367.0\n",
      "-16607684.0\n",
      "-16776772.0\n",
      "-16599369.0\n",
      "-16173479.0\n",
      "-17035430.0\n",
      "-15995398.0\n",
      "-16569506.0\n",
      "-16596309.0\n",
      "-16748730.0\n",
      "-16957748.0\n",
      "-17244350.0\n",
      "-16730371.0\n",
      "-16084205.0\n",
      "-16731268.0\n",
      "-16162152.0\n",
      "-16394034.0\n",
      "-17344100.0\n",
      "-16321757.0\n",
      "-16508630.0\n",
      "-16834748.0\n",
      "-16470039.0\n",
      "-16153776.0\n",
      "-16838646.0\n",
      "-17354556.0\n",
      "-16610435.0\n",
      "-17042362.0\n",
      "-16478731.0\n",
      "-16453244.0\n",
      "-16754570.0\n",
      "-15859600.0\n",
      "-16350082.0\n",
      "-16015367.0\n",
      "-16561790.0\n",
      "-17053204.0\n",
      "-17711452.0\n",
      "-16346984.0\n",
      "-16514302.0\n",
      "-16075684.0\n",
      "-16514518.0\n",
      "-16855790.0\n",
      "-16332950.0\n",
      "-17018886.0\n",
      "-16221620.0\n",
      "-16896390.0\n",
      "-18100200.0\n",
      "-16468659.0\n",
      "-16833216.0\n",
      "-16642976.0\n",
      "-16048143.0\n",
      "-16405050.0\n",
      "-16302005.0\n",
      "-16268425.0\n",
      "-16939120.0\n",
      "-16024856.0\n",
      "-16469208.0\n",
      "-15898847.0\n",
      "-16635433.0\n",
      "-16709218.0\n",
      "-15943022.0\n",
      "-16495235.0\n",
      "-17154208.0\n",
      "-16863674.0\n",
      "-17097072.0\n",
      "-17049414.0\n",
      "-16578296.0\n",
      "-16729036.0\n",
      "-16802778.0\n",
      "-16500842.0\n",
      "-16113734.0\n",
      "-16883376.0\n",
      "-16691275.0\n",
      "-16673466.0\n",
      "-16351008.0\n",
      "-16212353.0\n",
      "-16794560.0\n",
      "-16402794.0\n",
      "-16177227.0\n",
      "-15919008.0\n",
      "-16122106.0\n",
      "-16752092.0\n",
      "-16985014.0\n",
      "-15982982.0\n",
      "-17032348.0\n",
      "-17066726.0\n",
      "-15747226.0\n",
      "-16441556.0\n",
      "-16827724.0\n",
      "-16375481.0\n",
      "-16812448.0\n",
      "-16622460.0\n",
      "-16713476.0\n",
      "-16801590.0\n",
      "-16016946.0\n",
      "-16096183.0\n",
      "-16411679.0\n",
      "-16157421.0\n",
      "-17034882.0\n",
      "-16679102.0\n",
      "-15993764.0\n",
      "-16136469.0\n",
      "-15992682.0\n",
      "-16650482.0\n",
      "-16429301.0\n",
      "-17238836.0\n",
      "-16116664.0\n",
      "-15797125.0\n",
      "-16582362.0\n",
      "-16853176.0\n",
      "-16590846.0\n",
      "-16571132.0\n",
      "-16988984.0\n",
      "-16216069.0\n",
      "-16543913.0\n",
      "-17567000.0\n",
      "-16295046.0\n",
      "-16812486.0\n",
      "-16193687.0\n",
      "-16994732.0\n",
      "-16499516.0\n",
      "-17087672.0\n",
      "-16152982.0\n",
      "-16456031.0\n",
      "-16099523.0\n",
      "-16265252.0\n",
      "-15888558.0\n",
      "-16858402.0\n",
      "-15921657.0\n",
      "-17027104.0\n",
      "-16800954.0\n",
      "-16041512.0\n",
      "-16756542.0\n",
      "-16680398.0\n",
      "-16612885.0\n",
      "-16620532.0\n",
      "-16296294.0\n",
      "-16085010.0\n",
      "-16779952.0\n",
      "-16781626.0\n",
      "-16689108.0\n",
      "-16630406.0\n",
      "-16071301.0\n",
      "-16706392.0\n",
      "-16332783.0\n",
      "-16243776.0\n",
      "-16483260.0\n",
      "-16220110.0\n",
      "-16784846.0\n",
      "-16144378.0\n",
      "-16516043.0\n",
      "-16545897.0\n",
      "-16274368.0\n",
      "-16333437.0\n",
      "-16622108.0\n",
      "-16162792.0\n",
      "-15817166.0\n",
      "-16423167.0\n",
      "-16211967.0\n",
      "-16436413.0\n",
      "-17070892.0\n",
      "-17043414.0\n",
      "-16126079.0\n",
      "-16973234.0\n",
      "-16491024.0\n",
      "-16566512.0\n",
      "-16374364.0\n",
      "-16507903.0\n",
      "-16558592.0\n",
      "-16550906.0\n",
      "-16582718.0\n",
      "-16322930.0\n",
      "-16372227.0\n",
      "-16811538.0\n",
      "-16312444.0\n",
      "-16178913.0\n",
      "-16700315.0\n",
      "-16898980.0\n",
      "-16834038.0\n",
      "-16431779.0\n",
      "-16471121.0\n",
      "-16126301.0\n",
      "-16633353.0\n",
      "-16483427.0\n",
      "-16735375.0\n",
      "-16712119.0\n",
      "-17164274.0\n",
      "-16953276.0\n",
      "-16456409.0\n",
      "-16528139.0\n",
      "-16988144.0\n",
      "-17021138.0\n",
      "-17382408.0\n",
      "-16830764.0\n",
      "-16136673.0\n",
      "-16606186.0\n",
      "-16502651.0\n",
      "-16697756.0\n",
      "-16267850.0\n",
      "-16024057.0\n",
      "-16525733.0\n",
      "-16006436.0\n",
      "-16655206.0\n",
      "-16435434.0\n",
      "-15910428.0\n",
      "-16047393.0\n",
      "-15836651.0\n",
      "-16430092.0\n",
      "-16459400.0\n",
      "-16667523.0\n",
      "-16449476.0\n",
      "-17205762.0\n",
      "-15761661.0\n",
      "-17161566.0\n",
      "-16142967.0\n",
      "Accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# define the network\n",
    "model = MLP(3)\n",
    "# train the model\n",
    "train_model(train_dl, model)\n",
    "# evaluate the model\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "# make a single prediction (expect class=1)\n",
    "#row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = predict(row, model)\n",
    "print('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN model training\n",
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #for i, (images, labels) in enumerate(train_loader):\n",
    "    for i, (train_inputs, train_targets) in enumerate(train_dl):\n",
    "\n",
    "        optimizer.zero_grad() # Clear gradients\n",
    "        outputs = model(train_inputs) # Forward propagation\n",
    "        loss = error(outputs, train_targets) # Calculate softmax and cross entropy loss\n",
    "        loss.backward() # Calculating gradients\n",
    "        optimizer.step() # Update parameters\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        if count % 50 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            # Predict test dataset\n",
    "            for i, (test_inputs, test_targets) in enumerate(test_dl):\n",
    "            for images, labels in test_loader:\n",
    "                test = Variable(images.view(-1, 28*28))\n",
    "                outputs = model(test_inputs) # Forward propagation\n",
    "                predicted = outputs.detach().numpy()\n",
    "                total += len(labels) # Total number of labels\n",
    "                correct += (predicted == labels).sum() # Total correct predictions\n",
    "            \n",
    "            accuracy = 100.0 * correct.item() / total\n",
    "            \n",
    "            # store loss and iteration\n",
    "            loss_list.append(loss.data.item())\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "            if count % 500 == 0:\n",
    "                print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data.item(), accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
